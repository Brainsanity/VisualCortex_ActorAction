04/11: First Meeting
	1. Went through the project requirements
	2. Created a Github Repository: https://github.com/Brainsanity/VisualCortex_ActorAction.git

	3. Goal before next meeting: (3 days)
		1) Try out some basic models, e.g., resnet152 + linear	(Bin)
		2) Read papers and think about possible models
			Brian:	[2][6]
			Ruitao:	[1][5]
			Bin:	[4] & other possible models


04/14: Second meeting (actual meeting date: 04/15)
	1. Results of basic models tried
	2. Explain learned models from papers to others, in terms of our task
	3. Determine SOME current existing (simplified for classfication) models to replicate

	4. Goal before next meetinng: (2 days)
		1) Replicate current models to reach the benchmark requirement
			a. Joint learning of object and action detectors, 2017 (Bin)
			b. End-to-end joint semantic segmentation of actors and actions in video, 2018 (Zhexin)
			c. Resources
				https://zhuanlan.zhihu.com/p/31426458
				https://github.com/jwyang/faster-rcnn.pytorch
				https://github.com/facebookresearch/detectron2
		2) Build new models with some GODD novelty to reach the benchmark requirements (Ruitao, Bin, Zhexin)
			a. CAMs (Class Active Mapping)
				Learning Deep Features for Discriminative Localization. CVPR 2016
				Visualize what the CNN is looking and how CNN shifts its attention over time
			b. Attention + ResNet + RPN (Ruitao)
			c. 2018. BMVC. Actor-Action Semantic Segmentation withRegion Masks
			d. 2018. Group Normalization


04/17: Third meeting (4 days)
	1. Report on replicated current models (should be successful)
	2. Report progress of novel models	(should have preliminary results, may not reach the benchmark)

	3. Goal before next meeting:
		1) Reach the benchmark with novel models
		2) Experiments:
			PCD
				python train.py --net=per_class_detection --model_path=param_pcd_100ep --num_epochs=100
				python eval_on_val.py --net=per_class_detection --model_path=param_pcd_100ep --note=100ep
			
			PCSA.v1
				python train.py --net=per_class_soft_attention --version=1 --model_path=param_pcsa_v1_100ep --num_epochs=100
				python eval_on_val.py --net=per_class_soft_attention --version=1 --model_path=param_pcsa_v1_100ep --note=100ep

			PCSA.v2
				python train.py --net=per_class_soft_attention --version=2 --model_path=param_pcsa_v2_100ep --num_epochs=100
				python eval_on_val.py --net=per_class_soft_attention --version=2 --model_path=param_pcsa_v2_100ep --note=100ep

			PCSA.v3
				python train.py --net=per_class_soft_attention --version=3 --model_path=param_pcsa_v3_100ep --num_epochs=100
				python eval_on_val.py --net=per_class_soft_attention --version=3 --model_path=param_pcsa_v3_100ep --note=100ep

			PCD3D
				python train.py --net=R_2plus1_D --model_path=param_R_2plus1_D_100ep --num_epochs=100
				python eval_on_val.py --net=R_2plus1_D --model_path=param_R_2plus1_D_100ep --note=100ep
				
				5EP  	nframes=16 speed=1: 	Precision: 12.3 Recall: 10.1 F1: 10.5
				+5EP 	nframes=4  speed=2: 	Precision: 26.8 Recall: 31.2 F1: 26.9
				+23EP	nframes=4  speed=2:	Precision: 41.1 Recall: 56.7 F1: 44.8
				+18EP	nframes=4  speed=2:	Precision: 44.8 Recall: 57.7 F1: 47.7 train 61.1, 76.6, 65.3
				+21EP 	nframes=8  speed=1:	Precision: 56.4 Recall: 65.5 F1: 57.8 train 86.1, 96.7, 89.3
				


04/21: Fourth meeting
	1. Report on successful novel models

	2. Goal for next meeting: (2 days)
		1) Finish write & ppt


04/23:	Last meeting
	1. Review & Submit



Benchmark requirements:
		Precision: 23.8
		Recall: 30.5
		F1: 25.2


2. Understand the dataset:
	Since A2D dataset is too large to be trained on a single GPU, you only need to use a smaller portion of A2D.
	How to load dataset?
	What is the data structure? How to access data?

	loader/


3. Understand template code / baseline model of each task
	How to run evaluation, data loader
		network.py
		train.py
		eval_on_val.py
		cfg/
		
		utils/

		test_for_submission.py


3. Novelty of your method: 
	Note that this cannot be trivial (e.g., more training epochs, larger learning rate). Methods without good novelty will not receive good grades.
