AlexNet:
	Dropout:
		reduce overfittig
		in-network ensembling

	Data augmentation:
		label-preserving transformation of training data
		reduce overfitting


VGG-16/19:
	"very deep"
	modularized design
	stage-wise training: VGG-11 => VGG-13 => VGG-16


GoogleNet/Inception v1, v2, v3, ...
	Multiple branches
	Shortcuts
	Bottleneck


ResNets:
	Plain deep nets are difficult to find the opimal solution
	DeepNets + Identity:
		If identity were optimal mapping, easy to set weights as 0
		If optimal mapping was close to identity, easy to find small fluctuations
		Lower trainning & testing error


ResNeXt:
	uniform multi-branch <=> group-conv
	Better accuracy
	Better trade-off for high-capacity models


Inception-ResNet
	Szegedy et al. 2017

DenseNet
	Huang et al. CVPR 2017

Xception
	Chollet CVPR 2017

MobileNets
	Howard et al. 2017

ShuffleNet
	Zhang et al. 2017


CAMs (Class Active Mapping)
	B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16
	visualize what the CNN is looking and how CNN shifts its attention over time


Faster R-CNN


two-stream Faster R-CNN
	Joint learning of object and action detectors, ICCV, 2017


Conbine two-stream Faster R-CNN with 3D Conv???